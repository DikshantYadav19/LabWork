You are missing the concept of what is the impact of using dynamic scheduling on the OpenMP overhead. Dynamic scheduling is there to help load balance problems where each loop iteration can take different amount of time and static iteration distribution would most likely create work imbalance between the different threads. Work imbalance leads to wasted CPU time as the threads that finish earlier simply wait for the other threads to finish. Dynamic scheduling overcomes that by distributing loop chunks on a first come, first served basis. But this adds overhead, since the OpenMP runtime system has to implement bookkeeping on which iteration was given out and which not and has to implement some type of synchronisation. Also each thread must make at lest one call to the OpenMP runtime every time it finishes its iteration block and goes looking for another one. With static scheduling all iteration blocks are precomputed initially and then each thread runs over its part without any interaction with the OpenMP runtime environment.

The most crucial difference between static and dynamic scheduling is the iteration chunk size (i.e. the number of consecutive loop iterations that each thread does before seeking to do work in another part of the iteration space). If omitted, the chunk size with static scheduling defaults to #_of_iterations/#_of_threads while the default chunk size for dynamic scheduling is 1, i.e. each thread has to ask the OpenMP runtime for each iteration of the distributed loop.

What happens in your case is that without collapse(3) you have NNN iteration chunks of the outer loop and each thread runs NNN*NNN iterations (of the inner loops) before asking the OpenMP runtime for another iteration. When you collapse the loops, the number of iteration chunks grows to NNN*NNN*NNN, i.e. there are many more chunks and each thread is going to ask the OpenMP runtime for a chunk after each iteration.

This brings another problem when inner loops are collapsed with the outermost: it will happen that many threads would get iterations that have the same value of i, which would break the computation as the order of execution is not guaranteed and it might happen that the last thread that writes to res1[i] is not the one that executes the last iteration of both inner loops.
